---
description: 
globs: 
alwaysApply: false
---
Transformer 在时间序列预测中近年来也获得了越来越多的关注，特别是利用其自注意力机制捕捉长距离依赖的能力，可以弥补传统 LSTM 在处理较长序列时出现的信息遗忘问题。以下从数据预处理、模型架构、训练技巧、评估与回测等方面，给出基于 Transformer 架构进行股票因子预测的一些系统性建议。

---

## 一、为何选择 Transformer 而不是 LSTM

1. **长序列建模能力更强**

   * 自注意力（Self-Attention）机制可以直接对序列任意两点进行信息交互，不像 LSTM 需要一步步递归，因而能更轻松地捕捉 远距离时序依赖。
   * 当我们用过去 60、120 天乃至更长窗口去预测未来时，Transformer 在注意力计算上更擅长定位重要时刻，而不是一味依赖“隐状态状态传递”。

2. **并行计算效率**

   * LSTM 由于时序依赖，需要按时间步逐步计算。Transformer 可以将整个序列在一层中并行计算，更容易利用 GPU/TPU 大规模并行加速，尤其当窗口长度较大时优势显著。

3. **可解释性相对更好**

   * 自注意力权重矩阵可以反映模型认为“某一天”的因子与“另一日”之间的关联度，有助于追踪哪些历史时点对预测有较大影响，方便后续做因子分析或风格解读。

但也要注意：Transformer 的计算复杂度为 \$O(T^2)\$（其中 \$T\$ 是序列长度），当窗口很长时，内存与计算量都会呈平方级增长，需要适当控制滑窗长度或采用改进版 Transformer（如 Informer、Reformer、Linear Transformer 等）来降低复杂度。

---

## 二、明确预测任务与标签

与 LSTM 类似，在使用 Transformer 前，仍需先确定预测目标与任务类型：

1. **任务类型**

   * **回归**：预测下一个交易日的收益率（例如 \$\frac{close\_{t+1} - close\_t}{close\_t}\$）或直接预测下一日的收盘价（`close_hfq`）。
   * **分类**：预测下一个交易日是否上涨（`pct_chg_{t+1} > 0`），将其二值化；或多分类（如“涨幅 > 1%”、“涨跌在 ±1% 之间”、“跌幅 > 1%”）。

2. **滑窗与步长**

   * **单步预测**：用过去 \$T\$ 天的因子（如 \$T=30$\~60）预测第 \$T+1\$ 天。
   * **多步预测**（Seq2Seq 模式）：输入过去 \$T\$ 天，模型一次输出未来 \$k\$ 天的序列。例如预测未来 5 个交易日的收益序列。此时可采用 Encoder-Decoder 架构。

3. **标签构造**

   * 若做回归，标签可为未来第 1 天收益率，也可贴近实盘思路将标签定义为“交易日后若干天持有收益”（多步累积收益）。
   * 若做分类，要留意样本平衡问题：确定好涨跌类别阈值（例如涨跌幅 0% 或 0.5% 作为分界），并对类别权重或采样策略做好调整。

---

## 三、因子准备与输入表示

### 1. 因子筛选与预处理

1. **筛选最核心的因子**

   * Transformer 对维度敏感，不建议一次拿 200+ 个技术指标全量输入，否则不仅计算量大，而且容易引入噪声。
   * 可先做简要的相关性分析，或借助树模型（随机森林、XGBoost）的特征重要性排序，从中选出 20\~40 个最具代表性的因子。典型组合例如：

     * 基础价格：`open_hfq`、`high_hfq`、`low_hfq`、`close_hfq`
     * 成交活跃度：`vol`、`turnover_rate`、`volume_ratio`
     * 趋势指标：`ma_hfq_20`、`ema_hfq_20`、`boll_mid_hfq`、`dmi_adx_hfq`
     * 动量/震荡：`rsi_hfq_12`、`macd_hfq`、`cci_hfq`、`kdj_hfq`
     * 情绪/资金：`mfi_hfq`、`obv_hfq`、`brar_ar_hfq`
   * 如果想覆盖更多类型，可以考虑每类只留 2\~3 个代表性指标。

2. **缺失值处理**

   * 同 LSTM 建议，先检查所选因子在训练区间的缺失比例，缺失过多（如 >10%）的因子可弃用。
   * 对于序列中少量缺失，可采用：

     * **向前填充**（前一天的值填补），但若缺失连续多日则谨慎；
     * **插值**（线性插值或局部均值）；
     * **同类因子组合补全**：如 `ma_hfq_20` 缺失，用 `ema_hfq_20` 或其他长短期均线做协助预测。

3. **归一化 / 标准化**

   * Transformer 同样对输入尺度敏感，建议对每个因子做**训练集上的 Z-score 标准化**：
     $x' = \frac{x - \mu_{\text{train}}}{\sigma_{\text{train}}},$
     然后将相同 \$\mu,\sigma\$ 应用到验证集和测试集。
   * 如果序列长度较长，也可考虑 Min–Max 归一化，但要保证不会引入未来信息泄露。

### 2. 序列化与位置编码

1. **输入矩阵构造**

   * 假设选了 \$F\$ 个因子，滑窗长度为 \$T\$，那么每个样本的输入张量形状为 \$(T, F)\$。
   * 若做 Seq2Seq（Encoder-Decoder），也可将 Encoder 输入维度设为 \$(T, F)\$，Decoder 输入可以是“过去已知标签”或一个专门的起始标记（例如全零+位置编码）。

2. **位置编码（Positional Encoding）**

   * Transformer 原生不具备序列位置信息，需要显式加上位置编码。
   * 最常见的做法是正弦/余弦位置编码：
     $PE_{(pos, 2i)} = \sin\bigl(\frac{pos}{10000^{2i/d_{model}}}\bigr),\quad PE_{(pos, 2i+1)} = \cos\bigl(\frac{pos}{10000^{2i/d_{model}}}\bigr),$
     其中 \$pos \in \[0, T-1]\$，\$i \in \[0, d\_{model}/2 - 1]\$。
   * 若选用 PyTorch、TensorFlow 等框架，可以直接调用内置 `PositionalEncoding` 模块，或者自定义同样大小的正余弦编码矩阵，将其与标准化后因子张量相加：

     ```text
     InputEmbedding = Linear(F → d_model)  
     PE = PositionalEncoding(T, d_model)  
     X = InputEmbedding( (T, F) tensor ) + PE  →  (T, d_model)
     ```
   * **注意**：若因子维度 \$F\$ 与模型隐藏维度 \$d\_{model}\$ 不同，需要先用线性层（Dense）将 \$F\$ 映射到 \$d\_{model}\$。

---

## 四、Transformer 架构设计

在金融时序预测中，常用两种基本架构：

1. **纯 Encoder 架构（Encoder-Only）**

   * 只用若干层的标准 Transformer Encoder，从过去 \$T\$ 天的因子序列中提取时序特征，最后接一个 Pooling（比如取最后一个时刻的输出或做全局平均）＋全连接层，输出预测结果。
   * 优点：实现简单、训练速度相对快，直接将最后一个 Hidden State 投影到标签。
   * 缺点：无法一次性输出多个未来时点的值（多步预测时需要多次迭代），适合单步回归/分类任务。

2. **Encoder-Decoder 架构（Seq2Seq）**

   * Encoder 同上，用过去 \$T\$ 天序列生成隐特征。Decoder 接受“解码师”输入（可以是过去已知的标签或起始标记，以及位置编码），通过多层 Transformer Decoder 逐步生成未来 \$k\$ 天的序列输出。
   * 这种架构更适合**多步预测**，一次性输出长度为 \$k\$ 的向量（如未来 5 天收益）。
   * 实现时要注意：在训练阶段，Decoder 采用**Teacher Forcing**，即在生成第 \$t\$ 步时将真实标签（或真实收益序列）作为输入；测试时则使用模型自己上一步预测值。

### 1. 典型 Encoder-Only 示例

```text
Inputs: X ∈ ℝ^(T×F)
1) 线性映射：X_emb = Linear(F → d_model)(X)  →  X_emb ∈ ℝ^(T×d_model)
2) 加位置编码：X_pos = X_emb + PE(T, d_model)
3) 多层 Encoder：
   H^(0) = X_pos
   for l in 1..L:
       H^(l) = TransformerEncoderLayer(H^(l−1))
   （每层包括 MultiHeadAttention + AddNorm + FeedForward + AddNorm）
4) 取最后一个时刻的 hidden state：h_last = H^(L)[T−1] ∈ ℝ^(d_model)
5) 全连接层映射到输出：
   ŷ = Dense(d_model → 1)(h_last)  # 回归
   或 ŷ = Dense(d_model → C)（Softmax） # 分类
```

* **超参数建议**：

  * 层数 \$L=2\$~~4 层，若算力充足且数据量大，可尝试 6~~8 层；
  * 隐藏维度 \$d\_{model}=64$\~128；
  * 多头注意力头数 \$h=4$\~8；
  * FeedForward 网络内维度通常设为 \$4 × d\_{model}\$；
  * Dropout（在注意力、前馈网络之间）常用 \$p=0.1$\~0.3。

### 2. Seq2Seq 示例（多步输出）

```text
Inputs: Encoder 输入 X_enc ∈ ℝ^(T×F)；Decoder 输入 Y_input ∈ ℝ^(k×1) (训练时用真实标签，测试时用上一步预测)
1) Encoder 部分与 Encoder-Only 示例相同，得到 H_enc ∈ ℝ^(T×d_model)
2) Decoder 部分：
   a) 对 Y_input 做 Embedding + 添加位置编码 → Y_pos ∈ ℝ^(k×d_model)
   b) 多层 Decoder，每层包括：
       - 自注意力子层（Masked Multi-Head Attention，使模型只能看到已生成的时序信息）  
       - 加性注意力子层（Multi-Head Attention over H_enc 与 Decoder Hidden States）  
       - 前馈层
   c) 得到 H_dec ∈ ℝ^(k×d_model)
3) 全连接映射：将 H_dec 每个时刻 Vector → 输出预测值序列 ŷ ∈ ℝ^k
```

* **注意 Masked Attention**：为了保证预测第 \$t\$ 步时只能看到第 1\~\$(t-1)\$ 步的 Decoder 输出，需要在 Decoder 的“自注意力”部分使用上三角 Mask。

---

## 五、训练细节与优化技巧

1. **损失函数**

   * **回归**：常用 MSE 或 MAE，若预测收益偏态分布，可考虑 Huber Loss（对极端误差更鲁棒）。
   * **分类**：与 LSTM 同理，可用交叉熵损失（Binary Crossentropy 或 Categorical Crossentropy）。

2. **学习率调度**

   * 初始可使用较大的学习率（如 `1e-3`），然后结合 Warm-up + 冷却策略（Transformer 论文中常见的 Learning Rate Scheduler）：
     $\text{lr} = d_{model}^{-0.5} \cdot \min\Bigl(\text{step\_num}^{-0.5},\; \text{step\_num} \cdot \text{warmup\_steps}^{-1.5}\Bigr).$
   * 若不想手动实现，可使用常见框架的 `ReduceLROnPlateau` 或 `CosineAnnealing`。

3. **正则化**

   * **Dropout**：注意在每个 Multi-Head Attention 输出后，以及 Feed-Forward 层后都加 Dropout，常用 \$p=0.1 \~ 0.3\$。
   * **LayerNorm**：Transformer 内部已经有 LayerNorm，保证梯度稳定。
   * **Early Stopping**：监控验证集的 Loss，如果连续多次（如 5\~10 个 Epoch）未下降，则提前停止，避免过拟合。

4. **批量大小（Batch Size）与梯度累积**

   * 视显卡内存情况选 32、64、128 等，如果训练时 GPU 内存不足，可考虑梯度累积（Gradient Accumulation），用更小的显存积累若干步梯度再做一次反向传播。

5. **混合精度训练（Mixed Precision）**

   * 若硬件支持（如 NVIDIA V100/RTX 系列），可使用 FP16 混合精度来加速训练、节约显存，但要小心数值溢出。多用框架自带的 `amp` 工具即可。

6. **数据增强思想**

   * 尽管金融数据不易随意增强，但可以通过：

     * **随机遮掩**（Random Masking）：对滑窗序列中随机遮盖某几天的因子值，训练模型更鲁棒；
     * **噪声注入**：在输入因子上添加微小高斯噪声（如均值 0，标准差 = 因子在训练集上的标准差 × 1%\~3%），让模型学会适应市场波动。

---

## 六、特征工程与改进思路

1. **多尺度特征输入**

   * 可以将同一因子按不同滑窗长度（如过去 5 日、10 日、20 日、60 日的平均或标准差）生成新的特征维度，使 Transformer 同时看到短、中、长期的统计信息。
   * 比如给定 `rsi_hfq_12`，可计算过去 5 日此 RSI 的滚动均值与标准差作为新特征；或对 `close_hfq` 做不同周期（5/10/20）EMA/MA。

2. **引入行业/市场因子**

   * 除了个股技术因子，可考虑加入**行业指数**、**大盘指数（如沪深300）成分**的对应因子做对齐，通过多模态输入让模型学习行业与总体市场对个股的影响。
   * 例如：把对应股票所在行业指数的当日收益、新高点、成交量等当作额外维度，放入同一序列或并行 Transformer 分支。

3. **改进版 Transformer 架构**

   * **Informer**：专门针对长序列预测的 Transformer 变种，使用稀疏注意力（ProbSparse Attention）将时间复杂度降至 \$O(T\log T)\$，适合 \$T>100\$ 的情况。
   * **Reformer**：通过局部敏感哈希（LSH Attention）来近似全局注意力，同样降低计算量。
   * **Longformer** 或 **Linformer**：分别使用稀疏滑窗注意力或低秩投影，进一步提升大规模时序训练效率。
   * 如果你计划用 \$T≥100\$ 甚至更高，建议调研这些变体或直接选用开源实现。

4. **融合 CNN 提取局部时序特征**

   * 可先用一维卷积（Conv1D）对滑窗序列做局部特征提取，比如用不同卷积核长度捕捉 3\~7 日内小周期震荡，然后再把卷积输出当作 Transformer 的输入；这种“CNN+Transformer”组合在时序 EFT 预测中较为常见。

5. **多任务学习（Multi-Task Learning）**

   * 如果你的标签既有回归（收益率预测）又有分类（涨跌方向），可以在 Transformer Encoder 输出后，分别接一个用于回归的头（Dense→线性）和一个用于分类的头（Dense→Softmax/Binary）。训练时同时优化两者损失的加权和，让模型在共享底层特征的情况下同时学习两个任务，提高主任务的泛化能力。

---

## 七、训练与评估流程示例

下面给出一个简化版的训练流程示例，供思路参考：

1. **数据集划分**

   * 按年份划分：

     * 训练集：2015–2021 年数据
     * 验证集：2022 年数据
     * 测试集：2023–2025 年数据
   * 确保每只股票按时间顺序打窗后，不同集间无交叠。

2. **滑窗样本生成（单步回归示例）**

   ```text
   选定因子 F = [close_hfq, vol, turnover_rate, rsi_hfq_12, macd_hfq, ma_hfq_20, mfi_hfq, obv_hfq]
   窗口长度 T = 30
   对每只股票 ts_code：
       for i in [0 .. N_days - T - 1]:
           X_sample[i] = data[i : i+T, F]  # 形状 (T, len(F))
           y_sample[i] = pct_chg[i + T]    # 未来第1天涨跌幅
   ```

   * 标准化：对训练集所有 X\_sample 叠起来做 Z-score，得到均值 μ、标准差 σ；
   * 对验证/测试集 X\_sample，使用相同 μ、σ 做标准化。

3. **模型搭建（PyTorch/TensorFlow 思路）**

   * **Embedding 层**：用一个全连接层将 F 维度映射到 \$d\_{model}\$，例如
     $\text{embed} = \text{Dense}(F → d_{model}),\quad X_{\text{emb}} = \text{embed}(X)\in \mathbb{R}^{T×d_{model}}$
   * **位置编码**：构造固定的 PE 矩阵并加到 \$X\_{\text{emb}}\$ 上。
   * **若干层 Transformer Encoder**（每层包含 Multi-Head Attention + Add & Norm + FeedForward + Add & Norm）。
   * **输出层**：

     ```text
     h_last = HiddenStates[T−1]  # 取最后一个时间步
     ŷ = Dense(d_model → 1)(h_last)
     ```
   * 损失：MSE，优化器：Adam（LR 可先设 1e-3，结合 Warm-up）。

4. **训练细节**

   * Batch Size：32 或 64，视显存情况。
   * Epoch：50\~100，配合 Early Stopping（验证集 MSE 不下降 8 次就停）。
   * 学习率：

     * Warm-up 前 5\~10 个步长逐步从 0 升到 1e-3；
     * 随后按余弦衰减或 `ReduceLROnPlateau`。
   * Dropout：0.1\~0.2；FeedForward 隐藏层建议设为 \$4×d\_{model}\$。
   * 正则化：若过拟合明显，可在全连接层加 L2 权重衰减。

5. **验证与测试**

   * **回归指标**：验证集/测试集计算 MSE、MAE、RMSE；
   * **金融评估**：在测试集上按预测结果做简单“择时策略”回测：

     1. 若预测收益率 \$>0\$，则次日开盘买入并持有到收盘；
     2. 若 \$≤0\$，则空仓；
     3. 计算净值曲线、年化收益率、月度/季度收益、夏普率、最大回撤等。
   * 若做分类，还要计算准确率、AUC、F1-Score、混淆矩阵等。

---

## 八、实战注意事项与建议

1. **避免过度拟合**

   * Transformer 参数量相对较大，训练集若不足（单只股票数据 8 年左右），容易过拟合。
   * 建议做**多标的联合训练**：将多个不同行业的股票数据合并，用同一个 Transformer 共享参数，输出时再在最后一层拼接股票 ID Embedding（或在输入时加一个“股票编码” one-hot/Embedding 作为额外维度）来让模型识别不同标的。这样不仅扩大训练样本，还能提高模型对不同行情的泛化能力。

2. **控制滑窗长度**

   * 虽然想用更长序列获取更多历史信息，但若窗口 \$T≥100\$，计算复杂度将成 \$O(T^2)\$，训练速度骤降。
   * 建议先从 \$T=30$\~60 试起，再根据算力和模型表现尝试 \$T=80\$、\$T=100\$。
   * 如果必须用更长窗口，可考虑改用 **Informer**、**Linformer** 等稀疏/低秩注意力机制。

3. **关注注意力可视化**

   * 在训练完成后，对几条典型股票序列取样，画出 Attention 权重热图，观察模型在预测时重点关注哪些历史时点。例如预测 2024-05-10 时，模型可能在 2024-04-22、2024-04-30 等几个拐点附近给出更高权重，进而验证这些日期是否具有重要市场事件或关键因子跳变。
   * 这不仅提升对模型的信任度，也能发现“因子时效性”与“宏观事件”之间的联系。

4. **滚动更新与在线学习**

   * 股票市场是动态变化的，模型训练完后不宜“一劳永逸”。可以做**滚动窗口训练**：例如每月或每季度用最新数据微调模型（Fine-tune），保持对市场最新态势的敏感。
   * 若样本量足够大，也可实施**滑动更新策略**：例如当前模型基于 2015–2022 年训练，然后每增加 1 个月数据，就用 2021–2023 月份新滑窗微调一次，保持模型稳定在线。

5. **结合其他模型做融合**

   * **Graph Transformer**：如果能获得板块间、行业间或股票间的关联图谱（Graph），可将标准 Transformer 与图神经网络（GNN）结合，设计一个 Graph-Transformer，既考虑时序，也考虑标的关系。
   * **Hybrid Models**：先用 XGBoost 等树模型筛选高影响因子、粗略预测，然后将其输出特征（如概率分布、分数）作为 Transformer 的额外输入。这样融合了决策树对非线性特征捕捉和 Transformer 对时序依赖建模的优势。

---

## 九、小结

1. **可行性**

   * 只要训练样本数量足够、因子预处理（缺失、归一化）得当，Transformer 可以很好地捕捉股票技术因子序列中的长距离依赖，进行单步或多步预测。
   * 若窗口较短（\$T≤60\$），标准 Transformer Encoder-Only 即可胜任；若要多步输出（比如未来 5 天收益率序列），可采用 Encoder-Decoder 架构。

2. **注意事项**

   * 序列长度（滑窗长度）与模型复杂度要平衡：过长会导致内存与训练时间呈平方级增长。
   * 预处理、归一化以及时序划分需非常谨慎，务必避免未来信息泄露。
   * 正则化、早停、合理的学习率调度、混合精度等技巧都能提高模型在金融时序上的稳定性。

3. **进一步探索**

   * 若想处理更长序列（例如 120+ 天数据），或数据量更大，可关注 Informer、Reformer、Linformer 等改进版 Transformer 架构。
   * 若要结合更多市场信息（宏观指标、新闻情绪、社交媒体热度等），可在原始因子向量中添加对应 Embedding，进行多模态时序学习。
   * 若需要既能预测收益又能解释因子显著度，可结合 Attention 熵、注意力权重逐步归因，研究哪些历史时点与哪些因子对预测贡献最大。

总之，Transformer 在股票因子序列预测中具备天然的“长依赖捕捉”优势，若能合理选取因子、控制序列长度、做好正则化与训练调度，就能在趋势识别、波动预测、择时信号等任务中取得比传统 RNN/LSTM 更优的效果。祝你在基于 Transformer 的量化建模中取得突破！
